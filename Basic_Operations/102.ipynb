{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.180:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Basics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x113f8f790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download JDBC driver and put it into extensions folder of JAVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcHostname = \"\"\n",
    "jdbcDatabase = \"PSX\"\n",
    "username = \"SA\"\n",
    "password = \"\"\n",
    "jdbcUrl = \"jdbc:sqlserver://{0};database={1};user={2};password={3}\".format(jdbcHostname, jdbcDatabase, \n",
    "                                                                           username, password)\n",
    "connectionProperties = {\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushdown_query = \"(select * from universe where ticker = 'SEARL') uni\"\n",
    "df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-----+-----+-----+-----+-------+\n",
      "|               Date|ticker| Open| High|  Low|close| Volume|\n",
      "+-------------------+------+-----+-----+-----+-----+-------+\n",
      "|2005-12-31 00:00:00| SEARL| 30.0| 31.5| 31.2| 31.5| 3500.0|\n",
      "|2006-03-03 00:00:00| SEARL| 36.0| 35.6|34.75| 35.0| 7500.0|\n",
      "|2006-03-21 00:00:00| SEARL| 37.8| 37.5|36.65| 37.0| 4000.0|\n",
      "|2006-03-23 00:00:00| SEARL| 37.0| 37.0| 36.0| 36.5| 9000.0|\n",
      "|2006-03-26 00:00:00| SEARL| 36.5| 37.1| 36.5| 37.1| 2000.0|\n",
      "|2006-03-27 00:00:00| SEARL| 37.1| 37.0| 37.0| 37.0|    0.0|\n",
      "|2006-03-29 00:00:00| SEARL|36.95|36.95|36.95|36.95|  500.0|\n",
      "|2006-03-30 00:00:00| SEARL|36.95| 37.0| 37.0| 37.0|  500.0|\n",
      "|2006-04-02 00:00:00| SEARL| 37.0|37.15| 37.0|37.15| 5000.0|\n",
      "|2006-04-03 00:00:00| SEARL|37.15| 38.0|37.15|37.75|10500.0|\n",
      "|2006-04-04 00:00:00| SEARL|37.75|38.75| 36.6|38.55|10500.0|\n",
      "|2006-04-05 00:00:00| SEARL|38.55| 39.0| 38.5| 39.0|16000.0|\n",
      "|2006-04-06 00:00:00| SEARL| 39.0| 39.5|38.55| 39.0| 5000.0|\n",
      "|2006-04-09 00:00:00| SEARL| 39.0|39.85|38.55|38.55|12000.0|\n",
      "|2006-04-10 00:00:00| SEARL|38.55| 38.0| 38.0| 38.0|    0.0|\n",
      "|2006-04-12 00:00:00| SEARL|38.55| 38.0| 38.0| 38.0|    0.0|\n",
      "|2006-04-13 00:00:00| SEARL| 38.0| 38.0| 38.0| 38.0|  500.0|\n",
      "|2006-04-16 00:00:00| SEARL| 38.0| 38.0| 37.5|37.75| 5000.0|\n",
      "|2006-04-17 00:00:00| SEARL|37.75| 38.0| 37.1|37.25| 7000.0|\n",
      "|2006-04-18 00:00:00| SEARL|37.25| 38.9| 37.3| 38.1| 4000.0|\n",
      "+-------------------+------+-----+-----+-----+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|ticker|              Open|              High|               Low|             close|            Volume|\n",
      "+-------+------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|  3232|              3232|              3232|              3232|              3232|              3232|\n",
      "|   mean|  null|183.53226485148517|186.65519801980216|180.80273205445576|  183.465396039604|278851.58106435643|\n",
      "| stddev|  null|176.98825222982038|179.66578724810626|174.48428988669076|176.73419793942404|  452752.968930753|\n",
      "|    min| SEARL|              24.0|             24.15|             23.85|              24.0|               0.0|\n",
      "|    max| SEARL|             740.0|             761.0|            728.02|            739.23|         5788100.0|\n",
      "+-------+------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'ticker', 'Open', 'High', 'Low', 'close', 'Volume']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter DataFrame to get Column or DataFrame object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['ticker'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select('ticker'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ticker: string, Open: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ticker', 'Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|ticker|Open|\n",
      "+------+----+\n",
      "| SEARL|30.0|\n",
      "| SEARL|36.0|\n",
      "+------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['ticker', 'Open']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select(['ticker', 'Open']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get DF Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.datetime(2005, 12, 31, 0, 0), ticker='SEARL', Open=30.0, High=31.5, Low=31.2, close=31.5, Volume=3500.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.head(2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new column to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "|               Date|ticker|Open|High|  Low|close|Volume|  AOHLC|\n",
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "|2005-12-31 00:00:00| SEARL|30.0|31.5| 31.2| 31.5|3500.0|  31.05|\n",
      "|2006-03-03 00:00:00| SEARL|36.0|35.6|34.75| 35.0|7500.0|35.3375|\n",
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('AOHLC', (df['Open'] + df['High'] + df['Low'] + df['Close'])/4).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above addition of column was not permenant. See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+----+-----+-----+------+\n",
      "|               Date|ticker|Open|High|  Low|close|Volume|\n",
      "+-------------------+------+----+----+-----+-----+------+\n",
      "|2005-12-31 00:00:00| SEARL|30.0|31.5| 31.2| 31.5|3500.0|\n",
      "|2006-03-03 00:00:00| SEARL|36.0|35.6|34.75| 35.0|7500.0|\n",
      "+-------------------+------+----+----+-----+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign new df to incorporate changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohlc = df.withColumn('AOHLC', (df['Open'] + df['High'] + df['Low'] + df['Close'])/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "|               Date|ticker|Open|High|  Low|close|Volume|  AOHLC|\n",
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "|2005-12-31 00:00:00| SEARL|30.0|31.5| 31.2| 31.5|3500.0|  31.05|\n",
      "|2006-03-03 00:00:00| SEARL|36.0|35.6|34.75| 35.0|7500.0|35.3375|\n",
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ohlc.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Renaming is not permenant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+----+-----+-----+------+------------+\n",
      "|               Date|ticker|Open|High|  Low|close|Volume|Average_ohlc|\n",
      "+-------------------+------+----+----+-----+-----+------+------------+\n",
      "|2005-12-31 00:00:00| SEARL|30.0|31.5| 31.2| 31.5|3500.0|       31.05|\n",
      "|2006-03-03 00:00:00| SEARL|36.0|35.6|34.75| 35.0|7500.0|     35.3375|\n",
      "+-------------------+------+----+----+-----+-----+------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ohlc.withColumnRenamed('AOHLC', 'Average_ohlc').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "|               Date|ticker|Open|High|  Low|close|Volume|  AOHLC|\n",
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "|2005-12-31 00:00:00| SEARL|30.0|31.5| 31.2| 31.5|3500.0|  31.05|\n",
      "|2006-03-03 00:00:00| SEARL|36.0|35.6|34.75| 35.0|7500.0|35.3375|\n",
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ohlc.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register DataFrame as SQL TempView\n",
    "#### i) SQL can be used to query a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohlc.createOrReplaceTempView('prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_price = spark.sql('SELECT Date, Open FROM PRICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|               Date|Open|\n",
      "+-------------------+----+\n",
      "|2005-12-31 00:00:00|30.0|\n",
      "|2006-03-03 00:00:00|36.0|\n",
      "+-------------------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "open_price.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+----+-----+-----+------+\n",
      "|               Date|ticker|Open|High|  Low|close|Volume|\n",
      "+-------------------+------+----+----+-----+-----+------+\n",
      "|2005-12-31 00:00:00| SEARL|30.0|31.5| 31.2| 31.5|3500.0|\n",
      "|2006-03-03 00:00:00| SEARL|36.0|35.6|34.75| 35.0|7500.0|\n",
      "+-------------------+------+----+----+-----+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close < 100\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+-----+-----+-------+\n",
      "|               Date|ticker|  Open|  High|  Low|close| Volume|\n",
      "+-------------------+------+------+------+-----+-----+-------+\n",
      "|2008-04-23 00:00:00| SEARL|104.05|104.95|99.55|99.65|83000.0|\n",
      "|2008-04-25 00:00:00| SEARL|100.85|  99.9| 96.9| 98.0|36000.0|\n",
      "+-------------------+------+------+------+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Close'] < 100) \n",
    "          & (df['Open'] > 100)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_100 = df.filter((df['Close'] < 100) \n",
    "          & (df['Open'] > 100)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all the records as list of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2008, 4, 23, 0, 0), ticker='SEARL', Open=104.05, High=104.95, Low=99.55, close=99.65, Volume=83000.0),\n",
       " Row(Date=datetime.datetime(2008, 4, 25, 0, 0), ticker='SEARL', Open=100.85, High=99.9, Low=96.9, close=98.0, Volume=36000.0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+--------+----------+-----------+\n",
      "|ticker|max(Open)|max(High)|max(Low)|max(close)|max(Volume)|\n",
      "+------+---------+---------+--------+----------+-----------+\n",
      "| SEARL|    740.0|    761.0|  728.02|    739.23|  5788100.0|\n",
      "+------+---------+---------+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('ticker').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max(Open)|\n",
      "+---------+\n",
      "|    740.0|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Open':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|min(Open)|\n",
      "+---------+\n",
      "|     24.0|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Open': 'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushdown_query = \"(select * from universe where ticker IN ('SEARL', 'OGDC')) uni\"\n",
    "df_mult_tick = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby_obj = df_mult_tick.groupBy('ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|ticker|max(open)|\n",
      "+------+---------+\n",
      "|  OGDC|   287.84|\n",
      "| SEARL|    740.0|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_groupby_obj.agg({'open':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|ticker|min(open)|\n",
      "+------+---------+\n",
      "|  OGDC|    40.97|\n",
      "| SEARL|     24.0|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.groupBy('ticker').agg({'open':'min'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg,countDistinct,stddev,count,format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Avg ODGDC 2 DEC|\n",
      "+---------------+\n",
      "|         155.66|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.filter(df_mult_tick.ticker == 'OGDC').select(avg('High').alias('Average OGDC')) \\\n",
    ".select(format_number('Average OGDC', 2).alias('Avg ODGDC 2 DEC')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Avg ODGDC 3 DEC|\n",
      "+---------------+\n",
      "|        155.660|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.filter(df_mult_tick.ticker == 'OGDC').select(avg('High').alias('Average OGDC')) \\\n",
    ".select(format_number('Average OGDC', 3).alias('Avg ODGDC 3 DEC')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      Average OGDC|\n",
      "+------------------+\n",
      "|155.66028700906375|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.filter(df_mult_tick.ticker == 'OGDC').select(avg('High').alias('Average OGDC')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Count OGDC|\n",
      "+----------+\n",
      "|      3310|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.filter(df_mult_tick.ticker == 'OGDC').select(count('High').alias('Count OGDC')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Distinct Count OGDC|\n",
      "+-------------------+\n",
      "|               2007|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.filter(df_mult_tick.ticker == 'OGDC').select(countDistinct('High').alias('Distinct Count OGDC')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OrderBy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+---------+\n",
      "|               Date|ticker|  Open|  High|   Low| close|   Volume|\n",
      "+-------------------+------+------+------+------+------+---------+\n",
      "|2005-12-31 00:00:00|  OGDC| 115.5|121.25| 115.5|121.25|2183300.0|\n",
      "|2005-12-31 00:00:00| SEARL|  30.0|  31.5|  31.2|  31.5|   3500.0|\n",
      "|2006-03-03 00:00:00|  OGDC|164.95| 166.5| 162.6|164.25|4.76343E7|\n",
      "|2006-03-03 00:00:00| SEARL|  36.0|  35.6| 34.75|  35.0|   7500.0|\n",
      "|2006-03-21 00:00:00|  OGDC| 150.8|155.25| 151.8|154.95|8.90524E7|\n",
      "|2006-03-21 00:00:00| SEARL|  37.8|  37.5| 36.65|  37.0|   4000.0|\n",
      "|2006-03-23 00:00:00|  OGDC|154.95|162.65| 155.6|162.65| 1.1142E8|\n",
      "|2006-03-23 00:00:00| SEARL|  37.0|  37.0|  36.0|  36.5|   9000.0|\n",
      "|2006-03-26 00:00:00|  OGDC|162.65| 165.7| 156.1|156.65|9.31207E7|\n",
      "|2006-03-26 00:00:00| SEARL|  36.5|  37.1|  36.5|  37.1|   2000.0|\n",
      "|2006-03-27 00:00:00| SEARL|  37.1|  37.0|  37.0|  37.0|      0.0|\n",
      "|2006-03-27 00:00:00|  OGDC|156.65| 158.8|154.45|157.05|5.83585E7|\n",
      "|2006-03-29 00:00:00|  OGDC| 155.0|162.25|154.05|161.45|8.58887E7|\n",
      "|2006-03-29 00:00:00| SEARL| 36.95| 36.95| 36.95| 36.95|    500.0|\n",
      "|2006-03-30 00:00:00|  OGDC|161.45|162.15|156.75| 157.6|6.24362E7|\n",
      "|2006-03-30 00:00:00| SEARL| 36.95|  37.0|  37.0|  37.0|    500.0|\n",
      "|2006-04-02 00:00:00| SEARL|  37.0| 37.15|  37.0| 37.15|   5000.0|\n",
      "|2006-04-02 00:00:00|  OGDC| 157.6| 159.1| 156.8|157.45| 3.4254E7|\n",
      "|2006-04-03 00:00:00|  OGDC|157.45| 159.0|157.45|157.45|4486600.0|\n",
      "|2006-04-03 00:00:00| SEARL| 37.15|  38.0| 37.15| 37.75|  10500.0|\n",
      "+-------------------+------+------+------+------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.orderBy('Date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+---------+\n",
      "|               Date|ticker|  Open|  High|   Low| close|   Volume|\n",
      "+-------------------+------+------+------+------+------+---------+\n",
      "|2019-12-13 00:00:00|  OGDC|130.48|135.99|130.35|135.19|1.08541E7|\n",
      "|2019-12-13 00:00:00| SEARL|194.51|197.01|193.05|194.28|1201800.0|\n",
      "|2019-12-12 00:00:00|  OGDC| 128.0|131.24| 128.0|129.85|2669000.0|\n",
      "|2019-12-12 00:00:00| SEARL|198.99|199.45| 193.0|194.14| 668100.0|\n",
      "|2019-12-11 00:00:00|  OGDC|129.68|129.68|127.75|128.48|2355500.0|\n",
      "|2019-12-11 00:00:00| SEARL| 206.3| 206.8| 198.5|199.69| 901400.0|\n",
      "|2019-12-10 00:00:00|  OGDC| 131.7|131.75| 129.9|130.85|1834200.0|\n",
      "|2019-12-10 00:00:00| SEARL|200.62| 206.0| 199.5|204.36|1240700.0|\n",
      "|2019-12-09 00:00:00|  OGDC|131.67| 132.1| 130.7|131.57|2847700.0|\n",
      "|2019-12-09 00:00:00| SEARL|198.25| 203.5|197.05|200.22|1290500.0|\n",
      "|2019-12-06 00:00:00| SEARL|206.39| 207.0|197.05|198.71|1226900.0|\n",
      "|2019-12-06 00:00:00|  OGDC| 129.7| 131.5| 129.6|130.67|3383700.0|\n",
      "|2019-12-05 00:00:00|  OGDC| 129.9| 130.0|128.58|129.09|4062500.0|\n",
      "|2019-12-05 00:00:00| SEARL| 209.0|211.39|203.03|205.96|1935800.0|\n",
      "|2019-12-04 00:00:00|  OGDC|128.25| 129.0|127.06|128.36|2149700.0|\n",
      "|2019-12-04 00:00:00| SEARL|200.51| 207.5| 198.0|206.34|3118900.0|\n",
      "|2019-12-03 00:00:00| SEARL| 195.1|202.54| 195.0|199.07|4242500.0|\n",
      "|2019-12-03 00:00:00|  OGDC| 129.0| 130.0|127.03|127.57|3319200.0|\n",
      "|2019-12-02 00:00:00|  OGDC| 131.0| 131.0|127.12|128.58|5452300.0|\n",
      "|2019-12-02 00:00:00| SEARL| 184.5| 192.9| 183.0| 192.9|2771800.0|\n",
      "+-------------------+------+------+------+------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.orderBy(df_mult_tick['Date'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing With Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = spark.read.csv('NullData.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Acct: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- Debit: string (nullable = true)\n",
      " |-- Credit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+\n",
      "|Acct|month|Debit|Credit|\n",
      "+----+-----+-----+------+\n",
      "|   A|    1|  100|   200|\n",
      "|   A|    2|  200|   200|\n",
      "|   A|    3|  300|    10|\n",
      "|   B|    1|   10|   200|\n",
      "|   B|    3|   20|  null|\n",
      "|   C|    1| 1000|   100|\n",
      "|   C|    2|   10|  null|\n",
      "|   C|    3| null|  null|\n",
      "+----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop NA Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+\n",
      "|Acct|month|Debit|Credit|\n",
      "+----+-----+-----+------+\n",
      "|   A|    1|  100|   200|\n",
      "|   A|    2|  200|   200|\n",
      "|   A|    3|  300|    10|\n",
      "|   B|    1|   10|   200|\n",
      "|   C|    1| 1000|   100|\n",
      "+----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop NA Values with threeshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+\n",
      "|Acct|month|Debit|Credit|\n",
      "+----+-----+-----+------+\n",
      "|   A|    1|  100|   200|\n",
      "|   A|    2|  200|   200|\n",
      "|   A|    3|  300|    10|\n",
      "|   B|    1|   10|   200|\n",
      "|   B|    3|   20|  null|\n",
      "|   C|    1| 1000|   100|\n",
      "|   C|    2|   10|  null|\n",
      "|   C|    3| null|  null|\n",
      "+----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Each row must contain atleast 2 non null values to be not dropped\n",
    "null_df.na.drop(thresh =2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+\n",
      "|Acct|month|Debit|Credit|\n",
      "+----+-----+-----+------+\n",
      "|   A|    1|  100|   200|\n",
      "|   A|    2|  200|   200|\n",
      "|   A|    3|  300|    10|\n",
      "|   B|    1|   10|   200|\n",
      "|   B|    3|   20|  null|\n",
      "|   C|    1| 1000|   100|\n",
      "|   C|    2|   10|  null|\n",
      "+----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define subset to drop\n",
    "null_df.na.drop(subset = ['Debit']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_null_df = null_df.withColumn('Debit', null_df['Debit'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_null_df = converted_null_df.withColumn('Credit', converted_null_df['Credit'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+\n",
      "|Acct|month|Debit|Credit|\n",
      "+----+-----+-----+------+\n",
      "|   A|    1|  100|   200|\n",
      "|   A|    2|  200|   200|\n",
      "|   A|    3|  300|    10|\n",
      "|   B|    1|   10|   200|\n",
      "|   B|    3|   20|  null|\n",
      "|   C|    1| 1000|   100|\n",
      "|   C|    2|   10|  null|\n",
      "|   C|    3| null|  null|\n",
      "+----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "converted_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|Acct|   1|   2|   3|\n",
      "+----+----+----+----+\n",
      "|   B|  10|null|  20|\n",
      "|   C|1000|  10|null|\n",
      "|   A| 100| 200| 300|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "converted_null_df.groupby('Acct').pivot('Month').sum('Debit').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_data = converted_null_df.groupby('Acct').pivot('Month').sum('Debit', 'Credit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Renaming columns in above df\n",
    "#https://sparkbyexamples.com/spark/rename-a-column-on-spark-dataframes/\n",
    "renamed_conv_null_df = pivoted_data.withColumnRenamed('1_sum(CAST(Debit AS BIGINT))', 'M1D') \\\n",
    "                      .withColumnRenamed('1_sum(CAST(Credit AS BIGINT))', 'M1C') \\\n",
    "                      .withColumnRenamed('2_sum(CAST(Debit AS BIGINT))', 'M2D') \\\n",
    "                    .withColumnRenamed('2_sum(CAST(Credit AS BIGINT))', 'M2C') \\\n",
    "                    .withColumnRenamed('3_sum(CAST(Debit AS BIGINT))', 'M3D') \\\n",
    "                    .withColumnRenamed('3_sum(CAST(Credit AS BIGINT))', 'M3C') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Acct: string (nullable = true)\n",
      " |-- 1_sum(CAST(Debit AS BIGINT)): long (nullable = true)\n",
      " |-- 1_sum(CAST(Credit AS BIGINT)): long (nullable = true)\n",
      " |-- 2_sum(CAST(Debit AS BIGINT)): long (nullable = true)\n",
      " |-- 2_sum(CAST(Credit AS BIGINT)): long (nullable = true)\n",
      " |-- 3_sum(CAST(Debit AS BIGINT)): long (nullable = true)\n",
      " |-- 3_sum(CAST(Credit AS BIGINT)): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+----+----+----+----+\n",
      "|Acct| M1D|M1C| M2D| M2C| M3D| M3C|\n",
      "+----+----+---+----+----+----+----+\n",
      "|   B|  10|200|null|null|  20|null|\n",
      "|   C|1000|100|  10|null|null|null|\n",
      "|   A| 100|200| 200| 200| 300|  10|\n",
      "+----+----+---+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamed_conv_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-4ca68798d9db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrenamed_conv_null_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'M2D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'M2D'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'M2D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "renamed_conv_null_df.select('M2D').na.fill(0)(when(col('M2D') == 0, 1).otherwise('M2D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/39048229/spark-equivalent-of-if-then-else\n",
    "renamed_conv_null_df.select('M2D').na.fill(0) \\\n",
    ".withColumn('M2D', when(col('M2D') == 0, 1) \\\n",
    "            .otherwise(col('M2D'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+----+----+----+----+\n",
      "|Acct| M1D|M1C| M2D| M2C| M3D| M3C|\n",
      "+----+----+---+----+----+----+----+\n",
      "|   B|  10|200|null|null|  20|null|\n",
      "|   C|1000|100|  10|null|null|null|\n",
      "|   A| 100|200| 200| 200| 300|  10|\n",
      "+----+----+---+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamed_conv_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = renamed_conv_null_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+----+----+----+----+\n",
      "|Acct| M1D|M1C| M2D| M2C| M3D| M3C|\n",
      "+----+----+---+----+----+----+----+\n",
      "|   B|  10|200|null|null|  20|null|\n",
      "|   C|1000|100|  10|null|null|null|\n",
      "|   A| 100|200| 200| 200| 300|  10|\n",
      "+----+----+---+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Ratio', col('M2C')).fillna(0, subset=['Ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+----+----+----+----+-----+\n",
      "|Acct| M1D|M1C| M2D| M2C| M3D| M3C|Ratio|\n",
      "+----+----+---+----+----+----+----+-----+\n",
      "|   B|  10|200|null|null|  20|null|    0|\n",
      "|   C|1000|100|  10|null|null|null|    0|\n",
      "|   A| 100|200| 200| 200| 300|  10|  200|\n",
      "+----+----+---+----+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select(func.col('M2C'))['M2C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Resolved attribute(s) M2C#5401L missing from M3C#1178L,M1C#1146L,M2C#1162L,Acct#800,M3D#1170L,M2D#1154L,M1D#1138L,div#4939L,Ratio#4930L in operator !Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, M2C#5401L AS Ratio#5403L, div#4939L]. Attribute(s) with the same name appear in the operation: M2C. Please check if the right attribute(s) are used.;;\\n!Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, M2C#5401L AS Ratio#5403L, div#4939L]\\n+- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, Ratio#4930L, Ratio#4930L AS div#4939L]\\n   +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, coalesce(Ratio#4913L, cast(0.0 as bigint)) AS Ratio#4930L]\\n      +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, M2C#1162L AS Ratio#4913L]\\n         +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, 3_sum(CAST(Credit AS BIGINT))#1130L AS M3C#1178L]\\n            +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, 3_sum(CAST(Debit AS BIGINT))#1129L AS M3D#1170L, 3_sum(CAST(Credit AS BIGINT))#1130L]\\n               +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, 2_sum(CAST(Credit AS BIGINT))#1128L AS M2C#1162L, 3_sum(CAST(Debit AS BIGINT))#1129L, 3_sum(CAST(Credit AS BIGINT))#1130L]\\n                  +- Project [Acct#800, M1D#1138L, M1C#1146L, 2_sum(CAST(Debit AS BIGINT))#1127L AS M2D#1154L, 2_sum(CAST(Credit AS BIGINT))#1128L, 3_sum(CAST(Debit AS BIGINT))#1129L, 3_sum(CAST(Credit AS BIGINT))#1130L]\\n                     +- Project [Acct#800, M1D#1138L, 1_sum(CAST(Credit AS BIGINT))#1126L AS M1C#1146L, 2_sum(CAST(Debit AS BIGINT))#1127L, 2_sum(CAST(Credit AS BIGINT))#1128L, 3_sum(CAST(Debit AS BIGINT))#1129L, 3_sum(CAST(Credit AS BIGINT))#1130L]\\n                        +- Project [Acct#800, 1_sum(CAST(Debit AS BIGINT))#1125L AS M1D#1138L, 1_sum(CAST(Credit AS BIGINT))#1126L, 2_sum(CAST(Debit AS BIGINT))#1127L, 2_sum(CAST(Credit AS BIGINT))#1128L, 3_sum(CAST(Debit AS BIGINT))#1129L, 3_sum(CAST(Credit AS BIGINT))#1130L]\\n                           +- Project [Acct#800, __pivot_sum(CAST(`Debit` AS BIGINT)) AS `sum(CAST(``Debit`` AS BIGINT))`#1116[0] AS 1_sum(CAST(Debit AS BIGINT))#1125L, __pivot_sum(CAST(`Credit` AS BIGINT)) AS `sum(CAST(``Credit`` AS BIGINT))`#1124[0] AS 1_sum(CAST(Credit AS BIGINT))#1126L, __pivot_sum(CAST(`Debit` AS BIGINT)) AS `sum(CAST(``Debit`` AS BIGINT))`#1116[1] AS 2_sum(CAST(Debit AS BIGINT))#1127L, __pivot_sum(CAST(`Credit` AS BIGINT)) AS `sum(CAST(``Credit`` AS BIGINT))`#1124[1] AS 2_sum(CAST(Credit AS BIGINT))#1128L, __pivot_sum(CAST(`Debit` AS BIGINT)) AS `sum(CAST(``Debit`` AS BIGINT))`#1116[2] AS 3_sum(CAST(Debit AS BIGINT))#1129L, __pivot_sum(CAST(`Credit` AS BIGINT)) AS `sum(CAST(``Credit`` AS BIGINT))`#1124[2] AS 3_sum(CAST(Credit AS BIGINT))#1130L]\\n                              +- Aggregate [Acct#800], [Acct#800, pivotfirst(Month#801, sum(CAST(`Debit` AS BIGINT))#1107L, 1, 2, 3, 0, 0) AS __pivot_sum(CAST(`Debit` AS BIGINT)) AS `sum(CAST(``Debit`` AS BIGINT))`#1116, pivotfirst(Month#801, sum(CAST(`Credit` AS BIGINT))#1108L, 1, 2, 3, 0, 0) AS __pivot_sum(CAST(`Credit` AS BIGINT)) AS `sum(CAST(``Credit`` AS BIGINT))`#1124]\\n                                 +- Aggregate [Acct#800, Month#801], [Acct#800, Month#801, sum(cast(Debit#888 as bigint)) AS sum(CAST(`Debit` AS BIGINT))#1107L, sum(cast(Credit#893 as bigint)) AS sum(CAST(`Credit` AS BIGINT))#1108L]\\n                                    +- Project [Acct#800, month#801, Debit#888, cast(Credit#803 as int) AS Credit#893]\\n                                       +- Project [Acct#800, month#801, cast(Debit#802 as int) AS Debit#888, Credit#803]\\n                                          +- Relation[Acct#800,month#801,Debit#802,Credit#803] csv\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o762.withColumn.\n: org.apache.spark.sql.AnalysisException: Resolved attribute(s) M2C#5401L missing from M3C#1178L,M1C#1146L,M2C#1162L,Acct#800,M3D#1170L,M2D#1154L,M1D#1138L,div#4939L,Ratio#4930L in operator !Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, M2C#5401L AS Ratio#5403L, div#4939L]. Attribute(s) with the same name appear in the operation: M2C. Please check if the right attribute(s) are used.;;\n!Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, M2C#5401L AS Ratio#5403L, div#4939L]\n+- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, Ratio#4930L, Ratio#4930L AS div#4939L]\n   +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, coalesce(Ratio#4913L, cast(0.0 as bigint)) AS Ratio#4930L]\n      +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, M2C#1162L AS Ratio#4913L]\n         +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, 3_sum(CAST(Credit AS BIGINT))#1130L AS M3C#1178L]\n            +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, 3_sum(CAST(Debit AS BIGINT))#1129L AS M3D#1170L, 3_sum(CAST(Credit AS BIGINT))#1130L]\n               +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, 2_sum(CAST(Credit AS BIGINT))#1128L AS M2C#1162L, 3_sum(CAST(Debit AS BIGINT))#1129L, 3_sum(CAST(Credit AS BIGINT))#1130L]\n                  +- Project [Acct#800, M1D#1138L, M1C#1146L, 2_sum(CAST(Debit AS BIGINT))#1127L AS M2D#1154L, 2_sum(CAST(Credit AS BIGINT))#1128L, 3_sum(CAST(Debit AS BIGINT))#1129L, 3_sum(CAST(Credit AS BIGINT))#1130L]\n                     +- Project [Acct#800, M1D#1138L, 1_sum(CAST(Credit AS BIGINT))#1126L AS M1C#1146L, 2_sum(CAST(Debit AS BIGINT))#1127L, 2_sum(CAST(Credit AS BIGINT))#1128L, 3_sum(CAST(Debit AS BIGINT))#1129L, 3_sum(CAST(Credit AS BIGINT))#1130L]\n                        +- Project [Acct#800, 1_sum(CAST(Debit AS BIGINT))#1125L AS M1D#1138L, 1_sum(CAST(Credit AS BIGINT))#1126L, 2_sum(CAST(Debit AS BIGINT))#1127L, 2_sum(CAST(Credit AS BIGINT))#1128L, 3_sum(CAST(Debit AS BIGINT))#1129L, 3_sum(CAST(Credit AS BIGINT))#1130L]\n                           +- Project [Acct#800, __pivot_sum(CAST(`Debit` AS BIGINT)) AS `sum(CAST(``Debit`` AS BIGINT))`#1116[0] AS 1_sum(CAST(Debit AS BIGINT))#1125L, __pivot_sum(CAST(`Credit` AS BIGINT)) AS `sum(CAST(``Credit`` AS BIGINT))`#1124[0] AS 1_sum(CAST(Credit AS BIGINT))#1126L, __pivot_sum(CAST(`Debit` AS BIGINT)) AS `sum(CAST(``Debit`` AS BIGINT))`#1116[1] AS 2_sum(CAST(Debit AS BIGINT))#1127L, __pivot_sum(CAST(`Credit` AS BIGINT)) AS `sum(CAST(``Credit`` AS BIGINT))`#1124[1] AS 2_sum(CAST(Credit AS BIGINT))#1128L, __pivot_sum(CAST(`Debit` AS BIGINT)) AS `sum(CAST(``Debit`` AS BIGINT))`#1116[2] AS 3_sum(CAST(Debit AS BIGINT))#1129L, __pivot_sum(CAST(`Credit` AS BIGINT)) AS `sum(CAST(``Credit`` AS BIGINT))`#1124[2] AS 3_sum(CAST(Credit AS BIGINT))#1130L]\n                              +- Aggregate [Acct#800], [Acct#800, pivotfirst(Month#801, sum(CAST(`Debit` AS BIGINT))#1107L, 1, 2, 3, 0, 0) AS __pivot_sum(CAST(`Debit` AS BIGINT)) AS `sum(CAST(``Debit`` AS BIGINT))`#1116, pivotfirst(Month#801, sum(CAST(`Credit` AS BIGINT))#1108L, 1, 2, 3, 0, 0) AS __pivot_sum(CAST(`Credit` AS BIGINT)) AS `sum(CAST(``Credit`` AS BIGINT))`#1124]\n                                 +- Aggregate [Acct#800, Month#801], [Acct#800, Month#801, sum(cast(Debit#888 as bigint)) AS sum(CAST(`Debit` AS BIGINT))#1107L, sum(cast(Credit#893 as bigint)) AS sum(CAST(`Credit` AS BIGINT))#1108L]\n                                    +- Project [Acct#800, month#801, Debit#888, cast(Credit#803 as int) AS Credit#893]\n                                       +- Project [Acct#800, month#801, cast(Debit#802 as int) AS Debit#888, Credit#803]\n                                          +- Relation[Acct#800,month#801,Debit#802,Credit#803] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:43)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:369)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2258)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2225)\n\tat sun.reflect.GeneratedMethodAccessor81.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-3812fea82ac7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#https://blog.justalfred.com/html/PyGotham2016.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m renamed_conv_null_df = renamed_conv_null_df.withColumn('Ratio', renamed_conv_null_df.select('M2C') \\\n\u001b[0;32m----> 3\u001b[0;31m                                                        .fillna(0, subset=['M2C'])['M2C'])\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \"\"\"\n\u001b[1;32m   1989\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1990\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Resolved attribute(s) M2C#5401L missing from M3C#1178L,M1C#1146L,M2C#1162L,Acct#800,M3D#1170L,M2D#1154L,M1D#1138L,div#4939L,Ratio#4930L in operator !Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, M2C#5401L AS Ratio#5403L, div#4939L]. Attribute(s) with the same name appear in the operation: M2C. Please check if the right attribute(s) are used.;;\\n!Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, M2C#5401L AS Ratio#5403L, div#4939L]\\n+- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, Ratio#4930L, Ratio#4930L AS div#4939L]\\n   +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, coalesce(Ratio#4913L, cast(0.0 as bigint)) AS Ratio#4930L]\\n      +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, M3C#1178L, M2C#1162L AS Ratio#4913L]\\n         +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, M3D#1170L, 3_sum(CAST(Credit AS BIGINT))#1130L AS M3C#1178L]\\n            +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, M2C#1162L, 3_sum(CAST(Debit AS BIGINT))#1129L AS M3D#1170L, 3_sum(CAST(Credit AS BIGINT))#1130L]\\n               +- Project [Acct#800, M1D#1138L, M1C#1146L, M2D#1154L, 2_sum(CAST(Credit AS BIGINT))#1128L AS M2C#1162L, 3_sum(CAST(Debit AS BIGINT))#1129L, 3_sum(CAST(Credit AS BIGINT))#1130L]\\n                  +- Project [Acct#800, M1D#1138L, M1C#1146L, 2_sum(CAST(Debit AS BIGINT))#1127L AS M2D#1154L, 2_sum(CAST(Credit AS BIGINT))#1128L, 3_sum(CAST(Debit AS BIGINT))#1129L, 3_sum(CAST(Credit AS BIGINT))#1130L]\\n                     +- Project [Acct#800, M1D#1138L, 1_sum(CAST(Credit AS BIGINT))#1126L AS M1C#1146L, 2_sum(CAST(Debit AS BIGINT))#1127L, 2_sum(CAST(Credit AS BIGINT))#1128L, 3_sum(CAST(Debit AS BIGINT))#1129L, 3_sum(CAST(Credit AS BIGINT))#1130L]\\n                        +- Project [Acct#800, 1_sum(CAST(Debit AS BIGINT))#1125L AS M1D#1138L, 1_sum(CAST(Credit AS BIGINT))#1126L, 2_sum(CAST(Debit AS BIGINT))#1127L, 2_sum(CAST(Credit AS BIGINT))#1128L, 3_sum(CAST(Debit AS BIGINT))#1129L, 3_sum(CAST(Credit AS BIGINT))#1130L]\\n                           +- Project [Acct#800, __pivot_sum(CAST(`Debit` AS BIGINT)) AS `sum(CAST(``Debit`` AS BIGINT))`#1116[0] AS 1_sum(CAST(Debit AS BIGINT))#1125L, __pivot_sum(CAST(`Credit` AS BIGINT)) AS `sum(CAST(``Credit`` AS BIGINT))`#1124[0] AS 1_sum(CAST(Credit AS BIGINT))#1126L, __pivot_sum(CAST(`Debit` AS BIGINT)) AS `sum(CAST(``Debit`` AS BIGINT))`#1116[1] AS 2_sum(CAST(Debit AS BIGINT))#1127L, __pivot_sum(CAST(`Credit` AS BIGINT)) AS `sum(CAST(``Credit`` AS BIGINT))`#1124[1] AS 2_sum(CAST(Credit AS BIGINT))#1128L, __pivot_sum(CAST(`Debit` AS BIGINT)) AS `sum(CAST(``Debit`` AS BIGINT))`#1116[2] AS 3_sum(CAST(Debit AS BIGINT))#1129L, __pivot_sum(CAST(`Credit` AS BIGINT)) AS `sum(CAST(``Credit`` AS BIGINT))`#1124[2] AS 3_sum(CAST(Credit AS BIGINT))#1130L]\\n                              +- Aggregate [Acct#800], [Acct#800, pivotfirst(Month#801, sum(CAST(`Debit` AS BIGINT))#1107L, 1, 2, 3, 0, 0) AS __pivot_sum(CAST(`Debit` AS BIGINT)) AS `sum(CAST(``Debit`` AS BIGINT))`#1116, pivotfirst(Month#801, sum(CAST(`Credit` AS BIGINT))#1108L, 1, 2, 3, 0, 0) AS __pivot_sum(CAST(`Credit` AS BIGINT)) AS `sum(CAST(``Credit`` AS BIGINT))`#1124]\\n                                 +- Aggregate [Acct#800, Month#801], [Acct#800, Month#801, sum(cast(Debit#888 as bigint)) AS sum(CAST(`Debit` AS BIGINT))#1107L, sum(cast(Credit#893 as bigint)) AS sum(CAST(`Credit` AS BIGINT))#1108L]\\n                                    +- Project [Acct#800, month#801, Debit#888, cast(Credit#803 as int) AS Credit#893]\\n                                       +- Project [Acct#800, month#801, cast(Debit#802 as int) AS Debit#888, Credit#803]\\n                                          +- Relation[Acct#800,month#801,Debit#802,Credit#803] csv\\n'"
     ]
    }
   ],
   "source": [
    "#https://blog.justalfred.com/html/PyGotham2016.html\n",
    "renamed_conv_null_df = renamed_conv_null_df.withColumn('Ratio', renamed_conv_null_df.select('M2C') \\\n",
    "                                                       .fillna(0, subset=['M2C']))\n",
    "                         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+----+----+----+----+-----+---+\n",
      "|Acct| M1D|M1C| M2D| M2C| M3D| M3C|Ratio|div|\n",
      "+----+----+---+----+----+----+----+-----+---+\n",
      "|   B|  10|200|null|null|  20|null|    0|  0|\n",
      "|   C|1000|100|  10|null|null|null|    0|  0|\n",
      "|   A| 100|200| 200| 200| 300|  10|  200|200|\n",
      "+----+----+---+----+----+----+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamed_conv_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|M2D|\n",
      "+---+\n",
      "|  0|\n",
      "| 10|\n",
      "|200|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamed_conv_null_df.select('M2D').fillna(0, subset=['M2D']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_conv_null_df.select('M2D').na.fill(0) \\\n",
    ".withColumn('M2D', when(col('M2D') == 0, 1) \\\n",
    ".otherwise(col('M2D')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_conv_null_df.select('M2C').na.fill(0)['M2C'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
