{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.196:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Basics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1139aa910>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two ways to read data from RDBMS\n",
    "#### i) Download JDBC driver and put it into extensions folder of JAVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcHostname = \"192.168.0.169\"\n",
    "jdbcDatabase = \"PSX\"\n",
    "username = \"SA\"\n",
    "password = \"Yasin@0334\"\n",
    "jdbcUrl = \"jdbc:sqlserver://{0};database={1};user={2};password={3}\".format(jdbcHostname, jdbcDatabase, \n",
    "                                                                           username, password)\n",
    "connectionProperties = {\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushdown_query = \"(select * from universe where ticker = 'SEARL') uni\"\n",
    "df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-----+-----+-----+-----+-------+\n",
      "|               Date|ticker| Open| High|  Low|close| Volume|\n",
      "+-------------------+------+-----+-----+-----+-----+-------+\n",
      "|2005-12-31 00:00:00| SEARL| 30.0| 31.5| 31.2| 31.5| 3500.0|\n",
      "|2006-03-03 00:00:00| SEARL| 36.0| 35.6|34.75| 35.0| 7500.0|\n",
      "|2006-03-21 00:00:00| SEARL| 37.8| 37.5|36.65| 37.0| 4000.0|\n",
      "|2006-03-23 00:00:00| SEARL| 37.0| 37.0| 36.0| 36.5| 9000.0|\n",
      "|2006-03-26 00:00:00| SEARL| 36.5| 37.1| 36.5| 37.1| 2000.0|\n",
      "|2006-03-27 00:00:00| SEARL| 37.1| 37.0| 37.0| 37.0|    0.0|\n",
      "|2006-03-29 00:00:00| SEARL|36.95|36.95|36.95|36.95|  500.0|\n",
      "|2006-03-30 00:00:00| SEARL|36.95| 37.0| 37.0| 37.0|  500.0|\n",
      "|2006-04-02 00:00:00| SEARL| 37.0|37.15| 37.0|37.15| 5000.0|\n",
      "|2006-04-03 00:00:00| SEARL|37.15| 38.0|37.15|37.75|10500.0|\n",
      "|2006-04-04 00:00:00| SEARL|37.75|38.75| 36.6|38.55|10500.0|\n",
      "|2006-04-05 00:00:00| SEARL|38.55| 39.0| 38.5| 39.0|16000.0|\n",
      "|2006-04-06 00:00:00| SEARL| 39.0| 39.5|38.55| 39.0| 5000.0|\n",
      "|2006-04-09 00:00:00| SEARL| 39.0|39.85|38.55|38.55|12000.0|\n",
      "|2006-04-10 00:00:00| SEARL|38.55| 38.0| 38.0| 38.0|    0.0|\n",
      "|2006-04-12 00:00:00| SEARL|38.55| 38.0| 38.0| 38.0|    0.0|\n",
      "|2006-04-13 00:00:00| SEARL| 38.0| 38.0| 38.0| 38.0|  500.0|\n",
      "|2006-04-16 00:00:00| SEARL| 38.0| 38.0| 37.5|37.75| 5000.0|\n",
      "|2006-04-17 00:00:00| SEARL|37.75| 38.0| 37.1|37.25| 7000.0|\n",
      "|2006-04-18 00:00:00| SEARL|37.25| 38.9| 37.3| 38.1| 4000.0|\n",
      "+-------------------+------+-----+-----+-----+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''(select top 10 * from universe) uni'''\n",
    "\n",
    "mssql_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:sqlserver://192.168.0.169:1433;databaseName=PSX;\") \\\n",
    "    .option(\"dbtable\", query) \\\n",
    "    .option(\"user\", \"SA\") \\\n",
    "    .option(\"password\", \"Yasin@0334\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+-----+------+-------+\n",
      "|               Date|ticker|  Open|  High|  Low| close| Volume|\n",
      "+-------------------+------+------+------+-----+------+-------+\n",
      "|2005-12-31 00:00:00|   ABL|  85.7| 89.95| 87.5| 89.95| 5500.0|\n",
      "|2005-12-31 00:00:00|  ABOT| 154.0| 161.7|155.0| 161.7| 5100.0|\n",
      "|2005-12-31 00:00:00|  ACPL|  95.0| 99.75| 93.5| 99.75| 2400.0|\n",
      "|2005-12-31 00:00:00| ADAMS|  30.2| 31.05| 31.0|  31.0| 3500.0|\n",
      "|2005-12-31 00:00:00|  ADMM|204.75|214.95|195.0|214.95| 1700.0|\n",
      "|2005-12-31 00:00:00|  ADOS| 12.05|  12.5| 12.1|  12.5| 1500.0|\n",
      "|2005-12-31 00:00:00|  ADTM|  2.75|   2.6|  2.6|   2.6|  500.0|\n",
      "|2005-12-31 00:00:00|  AGIC|  35.0| 36.75|36.65| 36.75| 8000.0|\n",
      "|2005-12-31 00:00:00|  AGIL| 65.25|  66.0| 65.5|  65.5| 6000.0|\n",
      "|2005-12-31 00:00:00|  AGTL| 204.0| 214.2|206.0| 214.2|11000.0|\n",
      "+-------------------+------+------+------+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mssql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write DF to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('results.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1).write.csv(\"results_x01.csv\", sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.csv('result_x02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time:  0.2292470932006836\n"
     ]
    }
   ],
   "source": [
    "t0 = t.time()\n",
    "\n",
    "read_df = spark.read.csv('results.csv/part_one.csv', header = True)\n",
    "\n",
    "t1 = t.time()\n",
    "\n",
    "print('Exec time: ', t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+-----+-----+-----+-------+\n",
      "|                Date|ticker| Open| High|  Low|close| Volume|\n",
      "+--------------------+------+-----+-----+-----+-----+-------+\n",
      "|2005-12-31T00:00:...| SEARL| 30.0| 31.5| 31.2| 31.5| 3500.0|\n",
      "|2006-03-03T00:00:...| SEARL| 36.0| 35.6|34.75| 35.0| 7500.0|\n",
      "|2006-03-21T00:00:...| SEARL| 37.8| 37.5|36.65| 37.0| 4000.0|\n",
      "|2006-03-23T00:00:...| SEARL| 37.0| 37.0| 36.0| 36.5| 9000.0|\n",
      "|2006-03-26T00:00:...| SEARL| 36.5| 37.1| 36.5| 37.1| 2000.0|\n",
      "|2006-03-27T00:00:...| SEARL| 37.1| 37.0| 37.0| 37.0|    0.0|\n",
      "|2006-03-29T00:00:...| SEARL|36.95|36.95|36.95|36.95|  500.0|\n",
      "|2006-03-30T00:00:...| SEARL|36.95| 37.0| 37.0| 37.0|  500.0|\n",
      "|2006-04-02T00:00:...| SEARL| 37.0|37.15| 37.0|37.15| 5000.0|\n",
      "|2006-04-03T00:00:...| SEARL|37.15| 38.0|37.15|37.75|10500.0|\n",
      "|2006-04-04T00:00:...| SEARL|37.75|38.75| 36.6|38.55|10500.0|\n",
      "|2006-04-05T00:00:...| SEARL|38.55| 39.0| 38.5| 39.0|16000.0|\n",
      "|2006-04-06T00:00:...| SEARL| 39.0| 39.5|38.55| 39.0| 5000.0|\n",
      "|2006-04-09T00:00:...| SEARL| 39.0|39.85|38.55|38.55|12000.0|\n",
      "|2006-04-10T00:00:...| SEARL|38.55| 38.0| 38.0| 38.0|    0.0|\n",
      "|2006-04-12T00:00:...| SEARL|38.55| 38.0| 38.0| 38.0|    0.0|\n",
      "|2006-04-13T00:00:...| SEARL| 38.0| 38.0| 38.0| 38.0|  500.0|\n",
      "|2006-04-16T00:00:...| SEARL| 38.0| 38.0| 37.5|37.75| 5000.0|\n",
      "|2006-04-17T00:00:...| SEARL|37.75| 38.0| 37.1|37.25| 7000.0|\n",
      "|2006-04-18T00:00:...| SEARL|37.25| 38.9| 37.3| 38.1| 4000.0|\n",
      "+--------------------+------+-----+-----+-----+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
