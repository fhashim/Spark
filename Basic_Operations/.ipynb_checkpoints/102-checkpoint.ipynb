{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.180:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Basics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x111a20710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download JDBC driver and put it into extensions folder of JAVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcHostname = \"192.168.0.169\"\n",
    "jdbcDatabase = \"PSX\"\n",
    "username = \"SA\"\n",
    "password = \"Yasin@0334\"\n",
    "jdbcUrl = \"jdbc:sqlserver://{0};database={1};user={2};password={3}\".format(jdbcHostname, jdbcDatabase, \n",
    "                                                                           username, password)\n",
    "connectionProperties = {\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushdown_query = \"(select * from universe where ticker = 'SEARL') uni\"\n",
    "df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-----+-----+-----+-----+-------+\n",
      "|               Date|ticker| Open| High|  Low|close| Volume|\n",
      "+-------------------+------+-----+-----+-----+-----+-------+\n",
      "|2005-12-31 00:00:00| SEARL| 30.0| 31.5| 31.2| 31.5| 3500.0|\n",
      "|2006-03-03 00:00:00| SEARL| 36.0| 35.6|34.75| 35.0| 7500.0|\n",
      "|2006-03-21 00:00:00| SEARL| 37.8| 37.5|36.65| 37.0| 4000.0|\n",
      "|2006-03-23 00:00:00| SEARL| 37.0| 37.0| 36.0| 36.5| 9000.0|\n",
      "|2006-03-26 00:00:00| SEARL| 36.5| 37.1| 36.5| 37.1| 2000.0|\n",
      "|2006-03-27 00:00:00| SEARL| 37.1| 37.0| 37.0| 37.0|    0.0|\n",
      "|2006-03-29 00:00:00| SEARL|36.95|36.95|36.95|36.95|  500.0|\n",
      "|2006-03-30 00:00:00| SEARL|36.95| 37.0| 37.0| 37.0|  500.0|\n",
      "|2006-04-02 00:00:00| SEARL| 37.0|37.15| 37.0|37.15| 5000.0|\n",
      "|2006-04-03 00:00:00| SEARL|37.15| 38.0|37.15|37.75|10500.0|\n",
      "|2006-04-04 00:00:00| SEARL|37.75|38.75| 36.6|38.55|10500.0|\n",
      "|2006-04-05 00:00:00| SEARL|38.55| 39.0| 38.5| 39.0|16000.0|\n",
      "|2006-04-06 00:00:00| SEARL| 39.0| 39.5|38.55| 39.0| 5000.0|\n",
      "|2006-04-09 00:00:00| SEARL| 39.0|39.85|38.55|38.55|12000.0|\n",
      "|2006-04-10 00:00:00| SEARL|38.55| 38.0| 38.0| 38.0|    0.0|\n",
      "|2006-04-12 00:00:00| SEARL|38.55| 38.0| 38.0| 38.0|    0.0|\n",
      "|2006-04-13 00:00:00| SEARL| 38.0| 38.0| 38.0| 38.0|  500.0|\n",
      "|2006-04-16 00:00:00| SEARL| 38.0| 38.0| 37.5|37.75| 5000.0|\n",
      "|2006-04-17 00:00:00| SEARL|37.75| 38.0| 37.1|37.25| 7000.0|\n",
      "|2006-04-18 00:00:00| SEARL|37.25| 38.9| 37.3| 38.1| 4000.0|\n",
      "+-------------------+------+-----+-----+-----+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|ticker|              Open|              High|               Low|             close|            Volume|\n",
      "+-------+------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|  3232|              3232|              3232|              3232|              3232|              3232|\n",
      "|   mean|  null|183.53226485148517|186.65519801980216|180.80273205445576|  183.465396039604|278851.58106435643|\n",
      "| stddev|  null|176.98825222982038|179.66578724810626|174.48428988669076|176.73419793942404|  452752.968930753|\n",
      "|    min| SEARL|              24.0|             24.15|             23.85|              24.0|               0.0|\n",
      "|    max| SEARL|             740.0|             761.0|            728.02|            739.23|         5788100.0|\n",
      "+-------+------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'ticker', 'Open', 'High', 'Low', 'close', 'Volume']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter DataFrame to get Column or DataFrame object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['ticker'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select('ticker'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ticker: string, Open: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ticker', 'Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|ticker|Open|\n",
      "+------+----+\n",
      "| SEARL|30.0|\n",
      "| SEARL|36.0|\n",
      "+------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['ticker', 'Open']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select(['ticker', 'Open']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get DF Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.datetime(2005, 12, 31, 0, 0), ticker='SEARL', Open=30.0, High=31.5, Low=31.2, close=31.5, Volume=3500.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.head(2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new column to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "|               Date|ticker|Open|High|  Low|close|Volume|  AOHLC|\n",
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "|2005-12-31 00:00:00| SEARL|30.0|31.5| 31.2| 31.5|3500.0|  31.05|\n",
      "|2006-03-03 00:00:00| SEARL|36.0|35.6|34.75| 35.0|7500.0|35.3375|\n",
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('AOHLC', (df['Open'] + df['High'] + df['Low'] + df['Close'])/4).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above addition of column was not permenant. See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+----+-----+-----+------+\n",
      "|               Date|ticker|Open|High|  Low|close|Volume|\n",
      "+-------------------+------+----+----+-----+-----+------+\n",
      "|2005-12-31 00:00:00| SEARL|30.0|31.5| 31.2| 31.5|3500.0|\n",
      "|2006-03-03 00:00:00| SEARL|36.0|35.6|34.75| 35.0|7500.0|\n",
      "+-------------------+------+----+----+-----+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign new df to incorporate changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohlc = df.withColumn('AOHLC', (df['Open'] + df['High'] + df['Low'] + df['Close'])/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "|               Date|ticker|Open|High|  Low|close|Volume|  AOHLC|\n",
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "|2005-12-31 00:00:00| SEARL|30.0|31.5| 31.2| 31.5|3500.0|  31.05|\n",
      "|2006-03-03 00:00:00| SEARL|36.0|35.6|34.75| 35.0|7500.0|35.3375|\n",
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ohlc.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Renaming is not permenant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+----+-----+-----+------+------------+\n",
      "|               Date|ticker|Open|High|  Low|close|Volume|Average_ohlc|\n",
      "+-------------------+------+----+----+-----+-----+------+------------+\n",
      "|2005-12-31 00:00:00| SEARL|30.0|31.5| 31.2| 31.5|3500.0|       31.05|\n",
      "|2006-03-03 00:00:00| SEARL|36.0|35.6|34.75| 35.0|7500.0|     35.3375|\n",
      "+-------------------+------+----+----+-----+-----+------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ohlc.withColumnRenamed('AOHLC', 'Average_ohlc').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "|               Date|ticker|Open|High|  Low|close|Volume|  AOHLC|\n",
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "|2005-12-31 00:00:00| SEARL|30.0|31.5| 31.2| 31.5|3500.0|  31.05|\n",
      "|2006-03-03 00:00:00| SEARL|36.0|35.6|34.75| 35.0|7500.0|35.3375|\n",
      "+-------------------+------+----+----+-----+-----+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ohlc.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register DataFrame as SQL TempView\n",
    "#### i) SQL can be used to query a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohlc.createOrReplaceTempView('prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_price = spark.sql('SELECT Date, Open FROM PRICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|               Date|Open|\n",
      "+-------------------+----+\n",
      "|2005-12-31 00:00:00|30.0|\n",
      "|2006-03-03 00:00:00|36.0|\n",
      "+-------------------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "open_price.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----+----+-----+-----+------+\n",
      "|               Date|ticker|Open|High|  Low|close|Volume|\n",
      "+-------------------+------+----+----+-----+-----+------+\n",
      "|2005-12-31 00:00:00| SEARL|30.0|31.5| 31.2| 31.5|3500.0|\n",
      "|2006-03-03 00:00:00| SEARL|36.0|35.6|34.75| 35.0|7500.0|\n",
      "+-------------------+------+----+----+-----+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close < 100\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+-----+-----+-------+\n",
      "|               Date|ticker|  Open|  High|  Low|close| Volume|\n",
      "+-------------------+------+------+------+-----+-----+-------+\n",
      "|2008-04-23 00:00:00| SEARL|104.05|104.95|99.55|99.65|83000.0|\n",
      "|2008-04-25 00:00:00| SEARL|100.85|  99.9| 96.9| 98.0|36000.0|\n",
      "+-------------------+------+------+------+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Close'] < 100) \n",
    "          & (df['Open'] > 100)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_100 = df.filter((df['Close'] < 100) \n",
    "          & (df['Open'] > 100)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all the records as list of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2008, 4, 23, 0, 0), ticker='SEARL', Open=104.05, High=104.95, Low=99.55, close=99.65, Volume=83000.0),\n",
       " Row(Date=datetime.datetime(2008, 4, 25, 0, 0), ticker='SEARL', Open=100.85, High=99.9, Low=96.9, close=98.0, Volume=36000.0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+--------+----------+-----------+\n",
      "|ticker|max(Open)|max(High)|max(Low)|max(close)|max(Volume)|\n",
      "+------+---------+---------+--------+----------+-----------+\n",
      "| SEARL|    740.0|    761.0|  728.02|    739.23|  5788100.0|\n",
      "+------+---------+---------+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('ticker').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max(Open)|\n",
      "+---------+\n",
      "|    740.0|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Open':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|min(Open)|\n",
      "+---------+\n",
      "|     24.0|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Open': 'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushdown_query = \"(select * from universe where ticker IN ('SEARL', 'OGDC')) uni\"\n",
    "df_mult_tick = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby_obj = df_mult_tick.groupBy('ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|ticker|max(open)|\n",
      "+------+---------+\n",
      "|  OGDC|   287.84|\n",
      "| SEARL|    740.0|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_groupby_obj.agg({'open':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|ticker|min(open)|\n",
      "+------+---------+\n",
      "|  OGDC|    40.97|\n",
      "| SEARL|     24.0|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.groupBy('ticker').agg({'open':'min'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg,countDistinct,stddev,count,format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Avg ODGDC 2 DEC|\n",
      "+---------------+\n",
      "|         155.66|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.filter(df_mult_tick.ticker == 'OGDC').select(avg('High').alias('Average OGDC')) \\\n",
    ".select(format_number('Average OGDC', 2).alias('Avg ODGDC 2 DEC')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Avg ODGDC 3 DEC|\n",
      "+---------------+\n",
      "|        155.660|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.filter(df_mult_tick.ticker == 'OGDC').select(avg('High').alias('Average OGDC')) \\\n",
    ".select(format_number('Average OGDC', 3).alias('Avg ODGDC 3 DEC')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mult_tick.filter(df_mult_tick.ticker == 'OGDC').select(avg('High').alias('Average OGDC')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Count OGDC|\n",
      "+----------+\n",
      "|      3310|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.filter(df_mult_tick.ticker == 'OGDC').select(count('High').alias('Count OGDC')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Distinct Count OGDC|\n",
      "+-------------------+\n",
      "|               2007|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.filter(df_mult_tick.ticker == 'OGDC').select(countDistinct('High').alias('Distinct Count OGDC')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OrderBy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+---------+\n",
      "|               Date|ticker|  Open|  High|   Low| close|   Volume|\n",
      "+-------------------+------+------+------+------+------+---------+\n",
      "|2005-12-31 00:00:00|  OGDC| 115.5|121.25| 115.5|121.25|2183300.0|\n",
      "|2005-12-31 00:00:00| SEARL|  30.0|  31.5|  31.2|  31.5|   3500.0|\n",
      "|2006-03-03 00:00:00|  OGDC|164.95| 166.5| 162.6|164.25|4.76343E7|\n",
      "|2006-03-03 00:00:00| SEARL|  36.0|  35.6| 34.75|  35.0|   7500.0|\n",
      "|2006-03-21 00:00:00|  OGDC| 150.8|155.25| 151.8|154.95|8.90524E7|\n",
      "|2006-03-21 00:00:00| SEARL|  37.8|  37.5| 36.65|  37.0|   4000.0|\n",
      "|2006-03-23 00:00:00|  OGDC|154.95|162.65| 155.6|162.65| 1.1142E8|\n",
      "|2006-03-23 00:00:00| SEARL|  37.0|  37.0|  36.0|  36.5|   9000.0|\n",
      "|2006-03-26 00:00:00|  OGDC|162.65| 165.7| 156.1|156.65|9.31207E7|\n",
      "|2006-03-26 00:00:00| SEARL|  36.5|  37.1|  36.5|  37.1|   2000.0|\n",
      "|2006-03-27 00:00:00| SEARL|  37.1|  37.0|  37.0|  37.0|      0.0|\n",
      "|2006-03-27 00:00:00|  OGDC|156.65| 158.8|154.45|157.05|5.83585E7|\n",
      "|2006-03-29 00:00:00|  OGDC| 155.0|162.25|154.05|161.45|8.58887E7|\n",
      "|2006-03-29 00:00:00| SEARL| 36.95| 36.95| 36.95| 36.95|    500.0|\n",
      "|2006-03-30 00:00:00|  OGDC|161.45|162.15|156.75| 157.6|6.24362E7|\n",
      "|2006-03-30 00:00:00| SEARL| 36.95|  37.0|  37.0|  37.0|    500.0|\n",
      "|2006-04-02 00:00:00| SEARL|  37.0| 37.15|  37.0| 37.15|   5000.0|\n",
      "|2006-04-02 00:00:00|  OGDC| 157.6| 159.1| 156.8|157.45| 3.4254E7|\n",
      "|2006-04-03 00:00:00|  OGDC|157.45| 159.0|157.45|157.45|4486600.0|\n",
      "|2006-04-03 00:00:00| SEARL| 37.15|  38.0| 37.15| 37.75|  10500.0|\n",
      "+-------------------+------+------+------+------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.orderBy('Date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+---------+\n",
      "|               Date|ticker|  Open|  High|   Low| close|   Volume|\n",
      "+-------------------+------+------+------+------+------+---------+\n",
      "|2019-12-13 00:00:00|  OGDC|130.48|135.99|130.35|135.19|1.08541E7|\n",
      "|2019-12-13 00:00:00| SEARL|194.51|197.01|193.05|194.28|1201800.0|\n",
      "|2019-12-12 00:00:00|  OGDC| 128.0|131.24| 128.0|129.85|2669000.0|\n",
      "|2019-12-12 00:00:00| SEARL|198.99|199.45| 193.0|194.14| 668100.0|\n",
      "|2019-12-11 00:00:00|  OGDC|129.68|129.68|127.75|128.48|2355500.0|\n",
      "|2019-12-11 00:00:00| SEARL| 206.3| 206.8| 198.5|199.69| 901400.0|\n",
      "|2019-12-10 00:00:00|  OGDC| 131.7|131.75| 129.9|130.85|1834200.0|\n",
      "|2019-12-10 00:00:00| SEARL|200.62| 206.0| 199.5|204.36|1240700.0|\n",
      "|2019-12-09 00:00:00|  OGDC|131.67| 132.1| 130.7|131.57|2847700.0|\n",
      "|2019-12-09 00:00:00| SEARL|198.25| 203.5|197.05|200.22|1290500.0|\n",
      "|2019-12-06 00:00:00| SEARL|206.39| 207.0|197.05|198.71|1226900.0|\n",
      "|2019-12-06 00:00:00|  OGDC| 129.7| 131.5| 129.6|130.67|3383700.0|\n",
      "|2019-12-05 00:00:00|  OGDC| 129.9| 130.0|128.58|129.09|4062500.0|\n",
      "|2019-12-05 00:00:00| SEARL| 209.0|211.39|203.03|205.96|1935800.0|\n",
      "|2019-12-04 00:00:00|  OGDC|128.25| 129.0|127.06|128.36|2149700.0|\n",
      "|2019-12-04 00:00:00| SEARL|200.51| 207.5| 198.0|206.34|3118900.0|\n",
      "|2019-12-03 00:00:00| SEARL| 195.1|202.54| 195.0|199.07|4242500.0|\n",
      "|2019-12-03 00:00:00|  OGDC| 129.0| 130.0|127.03|127.57|3319200.0|\n",
      "|2019-12-02 00:00:00|  OGDC| 131.0| 131.0|127.12|128.58|5452300.0|\n",
      "|2019-12-02 00:00:00| SEARL| 184.5| 192.9| 183.0| 192.9|2771800.0|\n",
      "+-------------------+------+------+------+------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mult_tick.orderBy(df_mult_tick['Date'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing With Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = spark.read.csv('NullData.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Acct: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- Debit: string (nullable = true)\n",
      " |-- Credit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+\n",
      "|Acct|month|Debit|Credit|\n",
      "+----+-----+-----+------+\n",
      "|   A|    1|  100|   200|\n",
      "|   A|    2|  200|   200|\n",
      "|   A|    3|  300|    10|\n",
      "|   B|    1|   10|   200|\n",
      "|   B|    3|   20|  null|\n",
      "|   C|    1| 1000|   100|\n",
      "|   C|    2|   10|  null|\n",
      "|   C|    3| null|  null|\n",
      "+----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop NA Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+\n",
      "|Acct|month|Debit|Credit|\n",
      "+----+-----+-----+------+\n",
      "|   A|    1|  100|   200|\n",
      "|   A|    2|  200|   200|\n",
      "|   A|    3|  300|    10|\n",
      "|   B|    1|   10|   200|\n",
      "|   C|    1| 1000|   100|\n",
      "+----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop NA Values with threeshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+\n",
      "|Acct|month|Debit|Credit|\n",
      "+----+-----+-----+------+\n",
      "|   A|    1|  100|   200|\n",
      "|   A|    2|  200|   200|\n",
      "|   A|    3|  300|    10|\n",
      "|   B|    1|   10|   200|\n",
      "|   B|    3|   20|  null|\n",
      "|   C|    1| 1000|   100|\n",
      "|   C|    2|   10|  null|\n",
      "|   C|    3| null|  null|\n",
      "+----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Each row must contain atleast 2 non null values to be not dropped\n",
    "null_df.na.drop(thresh =2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+\n",
      "|Acct|month|Debit|Credit|\n",
      "+----+-----+-----+------+\n",
      "|   A|    1|  100|   200|\n",
      "|   A|    2|  200|   200|\n",
      "|   A|    3|  300|    10|\n",
      "|   B|    1|   10|   200|\n",
      "|   B|    3|   20|  null|\n",
      "|   C|    1| 1000|   100|\n",
      "|   C|    2|   10|  null|\n",
      "+----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define subset to drop\n",
    "null_df.na.drop(subset = ['Debit']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_null_df = null_df.withColumn('Debit', null_df['Debit'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_null_df = converted_null_df.withColumn('Credit', converted_null_df['Credit'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+------+\n",
      "|Acct|month|Debit|Credit|\n",
      "+----+-----+-----+------+\n",
      "|   A|    1|  100|   200|\n",
      "|   A|    2|  200|   200|\n",
      "|   A|    3|  300|    10|\n",
      "|   B|    1|   10|   200|\n",
      "|   B|    3|   20|  null|\n",
      "|   C|    1| 1000|   100|\n",
      "|   C|    2|   10|  null|\n",
      "|   C|    3| null|  null|\n",
      "+----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "converted_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|Acct|   1|   2|   3|\n",
      "+----+----+----+----+\n",
      "|   B|  10|null|  20|\n",
      "|   C|1000|  10|null|\n",
      "|   A| 100| 200| 300|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "converted_null_df.groupby('Acct').pivot('Month').sum('Debit').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_data = converted_null_df.groupby('Acct').pivot('Month').sum('Debit', 'Credit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Renaming columns in above df\n",
    "#https://sparkbyexamples.com/spark/rename-a-column-on-spark-dataframes/\n",
    "renamed_conv_null_df = pivoted_data.withColumnRenamed('1_sum(CAST(Debit AS BIGINT))', 'M1D') \\\n",
    "                      .withColumnRenamed('1_sum(CAST(Credit AS BIGINT))', 'M1C') \\\n",
    "                      .withColumnRenamed('2_sum(CAST(Debit AS BIGINT))', 'M2D') \\\n",
    "                    .withColumnRenamed('2_sum(CAST(Credit AS BIGINT))', 'M2C') \\\n",
    "                    .withColumnRenamed('3_sum(CAST(Debit AS BIGINT))', 'M3D') \\\n",
    "                    .withColumnRenamed('3_sum(CAST(Credit AS BIGINT))', 'M3C') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Acct: string (nullable = true)\n",
      " |-- 1_sum(CAST(Debit AS BIGINT)): long (nullable = true)\n",
      " |-- 1_sum(CAST(Credit AS BIGINT)): long (nullable = true)\n",
      " |-- 2_sum(CAST(Debit AS BIGINT)): long (nullable = true)\n",
      " |-- 2_sum(CAST(Credit AS BIGINT)): long (nullable = true)\n",
      " |-- 3_sum(CAST(Debit AS BIGINT)): long (nullable = true)\n",
      " |-- 3_sum(CAST(Credit AS BIGINT)): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+----+----+----+----+\n",
      "|Acct| M1D|M1C| M2D| M2C| M3D| M3C|\n",
      "+----+----+---+----+----+----+----+\n",
      "|   B|  10|200|null|null|  20|null|\n",
      "|   C|1000|100|  10|null|null|null|\n",
      "|   A| 100|200| 200| 200| 300|  10|\n",
      "+----+----+---+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamed_conv_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-4ca68798d9db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrenamed_conv_null_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'M2D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'M2D'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'M2D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "renamed_conv_null_df.select('M2D').na.fill(0)(when(col('M2D') == 0, 1).otherwise('M2D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|M2D|\n",
      "+---+\n",
      "|  1|\n",
      "| 10|\n",
      "|200|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/39048229/spark-equivalent-of-if-then-else\n",
    "renamed_conv_null_df.select('M2D').na.fill(0) \\\n",
    ".withColumn('M2D', when(col('M2D') == 0, 1) \\\n",
    "            .otherwise(col('M2D'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+----+----+----+----+\n",
      "|Acct| M1D|M1C| M2D| M2C| M3D| M3C|\n",
      "+----+----+---+----+----+----+----+\n",
      "|   B|  10|200|null|null|  20|null|\n",
      "|   C|1000|100|  10|null|null|null|\n",
      "|   A| 100|200| 200| 200| 300|  10|\n",
      "+----+----+---+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamed_conv_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "col should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-a301e1e6be22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#https://blog.justalfred.com/html/PyGotham2016.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m renamed_conv_null_df = renamed_conv_null_df.withColumn('Ratio', \n\u001b[0;32m----> 3\u001b[0;31m                                                       renamed_conv_null_df.select('M2C').na.fill(0))\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \"\"\"\n\u001b[0;32m-> 1989\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: col should be Column"
     ]
    }
   ],
   "source": [
    "#https://blog.justalfred.com/html/PyGotham2016.html\n",
    "renamed_conv_null_df = renamed_conv_null_df.withColumn('Ratio', \n",
    "                                                      renamed_conv_null_df.select('M2C').na.fill(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(renamed_conv_null_df.select('M2D').na.fill(0) \\\n",
    ".withColumn('M2D', when(col('M2D') == 0, 1) \\\n",
    ".otherwise(col('M2D')))['M2D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(renamed_conv_null_df.select('M2C').na.fill(0)['M2C'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
